{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget --header=\"Host: doc-08-0s-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en;q=0.9,en-US;q=0.8,hi;q=0.7\" --header=\"Cookie: AUTH_r22p3bnqkdovpg2br3nin4vq452ouvbq_nonce=ggaq77o489g9q\" --header=\"Connection: keep-alive\" \"https://doc-08-0s-docs.googleusercontent.com/docs/securesc/6t6i02ovbgfj1os3719d8cb4ahfv0ts1/jkst4npur2i0tf952t1osbopi17lukak/1662545850000/00484516897554883881/13665802893543793158/1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_?e=download&ax=AI9vYm62LGtBaR10zi2BF5a-rRXOQ--517ridUndk4QLGqSrDak8ryiQt2nLS_uw5yaqBe8gSqZnUW47dyXltGvgDLLvtGbWXvTq3vFdoCCHBtvYJsShukFLOa9NCztdJ1RISeizvJQs4OAfLML47jgPs5oeZkFOBdR1AYrMreqOjx_u-joRMi8xaqqx9zk32AyYa2g7r_cVj-qRzpWVkUPBfJNQqD0658ET0EydeChq_cbbAAk1FyOoLeJiKr3AMoZVfHBCoVfWllO7mJl2A0JWCyrfwhOQ-ZjeixZF9HkH-Jcpd75IuMBFteDUTY4njPd6WyzYB48Y0rcTg_eDacWF-duFV4HFL47QreWh5hon2KwMk1-MJxqvqjL9lz9zBt8YbjDEypdYYdqQCoXjf9VaqHL9mzkjr0NZvCP-FeC_FdJZr_-dR4Dyh3WmbQmOFA205zRQrkLUns9Xn6UQmlFv-rpQOyUsr8mLobdVMBOU82Lha_59p4oEcMh-M_tDeUvvkjcKOCYYI39_tQAFK-n__Tk8VwrlQ84CDUDGhIr9p54RDQi4xksFHweDEF7OLhlIsO_ZwbPpbI20T59otEm031zfku0s5y7RGUtCXCBbdcz8-GteIIcoxmR-eWPxQQXi-S6Z2krS9O5qBnNBVotWpKYJrw3phKiD0i7-IMDsmfQMesivCJPcaf2Q_S-GPy0q2mgZECV0cE9jjQIJTX6S_xH_gcC5TEKGjGJ_0rHsTdClcrR1SMvkARHT1WXD6-4emC1oxCOxFVvywSIQLP_KBRWbxk6SaPBW8ujEVC9QWBMmdK4cHmKMAw&uuid=ad0ad229-3c33-48a7-9ff3-aa2b0dce049e&authuser=0&nonce=ggaq77o489g9q&user=13665802893543793158&hash=ssgiujrvbtnnol2jochpc0be2qfpccq8\" -c -O 'glove_vectors'"
      ],
      "metadata": {
        "id": "tY36O-kRdbvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT47XA5hW5lZ",
        "outputId": "c7c6e9b5-6f70-4f88-9a2b-17e4cc34aada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import string \n",
        "import re\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "uCakMkc3ZF5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "SycTfeKVuiws"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_1(uploaded_file): \n",
        "     import pickle\n",
        "     with open(\"glove_vectors\",'rb') as f:\n",
        "              embeddings_index=pickle.load(f)\n",
        "     f.close()\n",
        "     model=pickle.load(open('file.pkl','rb'))\n",
        "     df=pd.read_csv(uploaded_file,index_col=\"Unnamed: 0\")\n",
        "     df.drop(columns=\"location\",inplace=True)\n",
        "     df.drop(columns=\"target\",inplace=True)\n",
        "    \n",
        "     df.dropna(inplace=True)\n",
        "     df.set_index(\"id\",inplace=True)\n",
        "     def word_count(text):\n",
        "         b=len(text.split())\n",
        "         return b\n",
        "\n",
        "     def count_char(text):\n",
        "         a=0\n",
        "         for i in text.split():\n",
        "           c=len(i)\n",
        "           a+=c   \n",
        "         return a\n",
        "          \n",
        "     def cap_count_char(text):\n",
        "         temp=0\n",
        "         for i in text:\n",
        "           if i.isupper():\n",
        "             temp+=1\n",
        "         return temp\n",
        "\n",
        "      \n",
        "     def punct(text):\n",
        "         p=string.punctuation\n",
        "         c=0\n",
        "         for i in p:\n",
        "           if i in text:\n",
        "              c+=1\n",
        "         return c\n",
        "\n",
        "     def uni_word(text):\n",
        "         uw=len(set(text.split()))\n",
        "         return uw\n",
        "        \n",
        "      \n",
        "     def hash(text):\n",
        "         m=re.findall(r'#\\w[A-Za-z0-9]*',text)\n",
        "         return len(m)\n",
        "\n",
        "     def ment(text):\n",
        "         m=re.findall(r'@\\w[A-Za-z0-9]*',text)\n",
        "         return len(m)\n",
        "\n",
        "     def number_sentence(text):\n",
        "         ns=nltk.sent_tokenize(text)\n",
        "         return len(ns)\n",
        "\n",
        "     df[\"ment\"]=df['text'].apply(lambda x:ment(x))\n",
        "     df[\"number_sentence\"]=df['text'].apply(lambda x:number_sentence(x))\n",
        "     df[\"hash\"]=df[\"text\"].apply(lambda x:hash(x))\n",
        "     df[\"uni_word\"]=df[\"text\"].apply(lambda x:uni_word(x))\n",
        "     df[\"punct\"]=df[\"text\"].apply(lambda x:punct(x))\n",
        "     df[\"cap_count_char\"]=df[\"text\"].apply(lambda x:cap_count_char(x))\n",
        "     df[\"count_char\"]=df[\"text\"].apply(lambda x:count_char(x))\n",
        "     df[\"word_count\"]=df[\"text\"].apply(lambda x:word_count(x))\n",
        "     df['average_word_length']=df['count_char']/df['word_count']\n",
        "     df['average_sentence_length']=df['word_count']/df['number_sentence']\n",
        "     df['unique_word_ratio']=df['uni_word']/df['word_count']\n",
        "     \n",
        "     lemmatizer = WordNetLemmatizer()\n",
        "     stopword=set(stopwords.words(\"english\"))\n",
        "     ps = PorterStemmer()\n",
        "     preprocessed_text=[]\n",
        "     for i in tqdm(df.text.values):\n",
        "       i=re.sub(r\"http\\S+\",\"\",i)\n",
        "       i=re.sub(\"\\S*\\d\\S*\",'',i)\n",
        "       i=re.sub(\"[^A-Za-z]+\",' ',i)\n",
        "       wk = word_tokenize(i)##https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "       i = ' '.join(w for w in wk if not w.lower() in stopword)\n",
        "        \n",
        "       preprocessed_text.append(i.strip())\n",
        "\n",
        "\n",
        "     df.text=preprocessed_text\n",
        "\n",
        "    \n",
        "     key=[]\n",
        "     for i in df[\"keyword\"]:\n",
        "         i=str(i)\n",
        "         a=re.sub(\"[^A-Za-z]+\",'',i)\n",
        "         key.append(a)\n",
        "     df[\"keyword\"]=key\n",
        "     \n",
        "     xtest=df.copy()\n",
        "     \n",
        "     vectorizer=pickle.load(open('vectorizer.pkl','rb'))\n",
        "     xtetex=vectorizer.transform(xtest[\"text\"].values)\n",
        "\n",
        "     labelencoder=pickle.load(open('labelencoder.pkl','rb'))\n",
        "     xtekwd=labelencoder.transform(xtest['keyword'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm11=pickle.load(open('norm11.pkl','rb'))\n",
        "     xteuwr=norm11.transform(xtest['unique_word_ratio'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm10=pickle.load(open('norm10.pkl','rb'))\n",
        "     xteasl=norm10.transform(xtest['average_sentence_length'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm9=pickle.load(open('norm9.pkl','rb'))\n",
        "     xteawl=norm9.transform(xtest['average_word_length'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm8=pickle.load(open('norm8.pkl','rb'))\n",
        "     xtewc=norm8.transform(xtest[\"word_count\"].values.reshape(-1,1))\n",
        "\n",
        "     norm7=pickle.load(open('norm7.pkl','rb'))\n",
        "     xtech=norm7.transform(xtest[\"count_char\"].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm6=pickle.load(open('norm6.pkl','rb'))\n",
        "     xtecc=norm6.transform(xtest[\"cap_count_char\"].values.reshape(-1,1))\n",
        "\n",
        "     norm5=pickle.load(open('norm5.pkl','rb'))\n",
        "     xtep=norm5.transform(xtest[\"punct\"].values.reshape(-1,1))\n",
        "\n",
        "     norm4=pickle.load(open('norm4.pkl','rb'))\n",
        "     xteuw=norm4.transform(xtest[\"uni_word\"].values.reshape(-1,1))\n",
        "\n",
        "     norm3=pickle.load(open('norm3.pkl','rb'))\n",
        "     xteh=norm3.transform(xtest[\"hash\"].values.reshape(-1,1))\n",
        "\n",
        "     norm1=pickle.load(open('norm1.pkl','rb'))\n",
        "     xtem=norm1.transform(xtest[\"ment\"].values.reshape(-1,1))\n",
        "\n",
        "     norm2=pickle.load(open('norm2.pkl','rb'))\n",
        "     xtens=norm2.transform(xtest[\"number_sentence\"].values.reshape(-1,1))\n",
        "     xte_t=xtest[\"text\"]\n",
        "     \n",
        "     tokenizer=pickle.load(open('tokenizer11.pkl','rb'))\n",
        "     vocab_size=len(tokenizer.word_index)+1\n",
        "     maxlen_text=21\n",
        "     embedding_matrix=np.zeros((vocab_size,300))\n",
        "     for word,i in tokenizer.word_index.items():\n",
        "        embedding_vector=embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          embedding_matrix[i]=embedding_vector\n",
        "\n",
        "     test_seq_text=tokenizer.texts_to_sequences(xte_t)\n",
        "     test_pad_text=pad_sequences(test_seq_text,maxlen=maxlen_text,padding='pre')\n",
        "\n",
        "\n",
        "     xte_k=xtest[\"keyword\"]\n",
        "     tokenizer=pickle.load(open('tokenizer21.pkl','rb'))\n",
        "     vocab_size_key=len(tokenizer.word_index)+1\n",
        "     maxlen_keyword=1\n",
        "     test_seq_keyword=tokenizer.texts_to_sequences(xte_k)\n",
        "     test_pad_keyword=pad_sequences(test_seq_keyword,maxlen=maxlen_keyword,padding='pre')\n",
        "\n",
        "     xte_com=np.concatenate((xteuwr,xteasl,xteawl,xtewc,xtech,xtecc,xtep,xteuw,xteh,xtem,xtens),axis=1)\n",
        "     from sklearn.metrics import f1_score\n",
        "     xte_final=[test_pad_text,test_pad_keyword,xte_com]\n",
        "     y_p=model.predict(xte_final)\n",
        "     if y_p >=0.5:\n",
        "       print(\"IT IS A DISASTER TWEET\")\n",
        "     else:\n",
        "       print(\"IT IS NOT A  DISASTER TWEET\")\n",
        " "
      ],
      "metadata": {
        "id": "LDunXatkYmGp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_1(\"pos1.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIUXVlIhZSPd",
        "outputId": "8b45c200-8b4f-4f5b-ca6e-6d45725b3d47"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 167.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IT IS A DISASTER TWEET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_2(uploaded_file):\n",
        "     import pickle\n",
        "     with open(\"glove_vectors\",'rb') as f:\n",
        "           embeddings_index=pickle.load(f)\n",
        "     f.close()\n",
        "     model=pickle.load(open('file.pkl','rb'))\n",
        "     df=pd.read_csv(uploaded_file)\n",
        "     df.drop(columns=\"location\",inplace=True)\n",
        "     df.dropna(inplace=True)\n",
        "     df.set_index(\"id\",inplace=True)\n",
        "     def word_count(text):\n",
        "         b=len(text.split())\n",
        "         return b\n",
        "\n",
        "     def count_char(text):\n",
        "         a=0\n",
        "         for i in text.split():\n",
        "           c=len(i)\n",
        "           a+=c   \n",
        "         return a\n",
        "          \n",
        "     def cap_count_char(text):\n",
        "         temp=0\n",
        "         for i in text:\n",
        "           if i.isupper():\n",
        "             temp+=1\n",
        "         return temp\n",
        "\n",
        "      \n",
        "     def punct(text):\n",
        "         p=string.punctuation\n",
        "         c=0\n",
        "         for i in p:\n",
        "           if i in text:\n",
        "              c+=1\n",
        "         return c\n",
        "\n",
        "     def uni_word(text):\n",
        "         uw=len(set(text.split()))\n",
        "         return uw\n",
        "        \n",
        "      \n",
        "     def hash(text):\n",
        "         m=re.findall(r'#\\w[A-Za-z0-9]*',text)\n",
        "         return len(m)\n",
        "\n",
        "     def ment(text):\n",
        "         m=re.findall(r'@\\w[A-Za-z0-9]*',text)\n",
        "         return len(m)\n",
        "\n",
        "     def number_sentence(text):\n",
        "         ns=nltk.sent_tokenize(text)\n",
        "         return len(ns)\n",
        "\n",
        "     df[\"ment\"]=df['text'].apply(lambda x:ment(x))\n",
        "     df[\"number_sentence\"]=df['text'].apply(lambda x:number_sentence(x))\n",
        "     df[\"hash\"]=df[\"text\"].apply(lambda x:hash(x))\n",
        "     df[\"uni_word\"]=df[\"text\"].apply(lambda x:uni_word(x))\n",
        "     df[\"punct\"]=df[\"text\"].apply(lambda x:punct(x))\n",
        "     df[\"cap_count_char\"]=df[\"text\"].apply(lambda x:cap_count_char(x))\n",
        "     df[\"count_char\"]=df[\"text\"].apply(lambda x:count_char(x))\n",
        "     df[\"word_count\"]=df[\"text\"].apply(lambda x:word_count(x))\n",
        "     df['average_word_length']=df['count_char']/df['word_count']\n",
        "     df['average_sentence_length']=df['word_count']/df['number_sentence']\n",
        "     df['unique_word_ratio']=df['uni_word']/df['word_count']\n",
        "     \n",
        "     lemmatizer = WordNetLemmatizer()\n",
        "     stopword=set(stopwords.words(\"english\"))\n",
        "     ps = PorterStemmer()\n",
        "     preprocessed_text=[]\n",
        "     for i in tqdm(df.text.values):\n",
        "       i=re.sub(r\"http\\S+\",\"\",i)\n",
        "       i=re.sub(\"\\S*\\d\\S*\",'',i)\n",
        "       i=re.sub(\"[^A-Za-z]+\",' ',i)\n",
        "       wk = word_tokenize(i)##https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "       i = ' '.join(w for w in wk if not w.lower() in stopword)\n",
        "        \n",
        "       preprocessed_text.append(i.strip())\n",
        "\n",
        "\n",
        "     df.text=preprocessed_text\n",
        "\n",
        "    \n",
        "     key=[]\n",
        "     for i in df[\"keyword\"]:\n",
        "         i=str(i)\n",
        "         a=re.sub(\"[^A-Za-z]+\",'',i)\n",
        "         key.append(a)\n",
        "     df[\"keyword\"]=key\n",
        "     \n",
        "     from sklearn.model_selection import train_test_split\n",
        "     y=df[\"target\"]\n",
        "     x=df.drop(\"target\",axis=1)\n",
        "     xtrain,xtest,ytrain,ytest=train_test_split(x,y,stratify=y,test_size=0.33,random_state=42)\n",
        "     \n",
        "     vectorizer=pickle.load(open('vectorizer.pkl','rb'))\n",
        "     xtetex=vectorizer.transform(xtest[\"text\"].values)\n",
        "\n",
        "     labelencoder=pickle.load(open('labelencoder.pkl','rb'))\n",
        "     xtekwd=labelencoder.transform(xtest['keyword'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm11=pickle.load(open('norm11.pkl','rb'))\n",
        "     xteuwr=norm11.transform(xtest['unique_word_ratio'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm10=pickle.load(open('norm10.pkl','rb'))\n",
        "     xteasl=norm10.transform(xtest['average_sentence_length'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm9=pickle.load(open('norm9.pkl','rb'))\n",
        "     xteawl=norm9.transform(xtest['average_word_length'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm8=pickle.load(open('norm8.pkl','rb'))\n",
        "     xtewc=norm8.transform(xtest[\"word_count\"].values.reshape(-1,1))\n",
        "\n",
        "     norm7=pickle.load(open('norm7.pkl','rb'))\n",
        "     xtech=norm7.transform(xtest[\"count_char\"].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm6=pickle.load(open('norm6.pkl','rb'))\n",
        "     xtecc=norm6.transform(xtest[\"cap_count_char\"].values.reshape(-1,1))\n",
        "\n",
        "     norm5=pickle.load(open('norm5.pkl','rb'))\n",
        "     xtep=norm5.transform(xtest[\"punct\"].values.reshape(-1,1))\n",
        "\n",
        "     norm4=pickle.load(open('norm4.pkl','rb'))\n",
        "     xteuw=norm4.transform(xtest[\"uni_word\"].values.reshape(-1,1))\n",
        "\n",
        "     norm3=pickle.load(open('norm3.pkl','rb'))\n",
        "     xteh=norm3.transform(xtest[\"hash\"].values.reshape(-1,1))\n",
        "\n",
        "     norm1=pickle.load(open('norm1.pkl','rb'))\n",
        "     xtem=norm1.transform(xtest[\"ment\"].values.reshape(-1,1))\n",
        "\n",
        "     norm2=pickle.load(open('norm2.pkl','rb'))\n",
        "     xtens=norm2.transform(xtest[\"number_sentence\"].values.reshape(-1,1))\n",
        "     xte_t=xtest[\"text\"]\n",
        "     \n",
        "     tokenizer=pickle.load(open('tokenizer11.pkl','rb'))\n",
        "     vocab_size=len(tokenizer.word_index)+1\n",
        "     maxlen_text=21\n",
        "     embedding_matrix=np.zeros((vocab_size,300))\n",
        "     for word,i in tokenizer.word_index.items():\n",
        "        embedding_vector=embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          embedding_matrix[i]=embedding_vector\n",
        "\n",
        "     test_seq_text=tokenizer.texts_to_sequences(xte_t)\n",
        "     test_pad_text=pad_sequences(test_seq_text,maxlen=maxlen_text,padding='pre')\n",
        "\n",
        "\n",
        "     xte_k=xtest[\"keyword\"]\n",
        "     tokenizer=pickle.load(open('tokenizer21.pkl','rb'))\n",
        "     vocab_size_key=len(tokenizer.word_index)+1\n",
        "     maxlen_keyword=1\n",
        "     test_seq_keyword=tokenizer.texts_to_sequences(xte_k)\n",
        "     test_pad_keyword=pad_sequences(test_seq_keyword,maxlen=maxlen_keyword,padding='pre')\n",
        "\n",
        "     xte_com=np.concatenate((xteuwr,xteasl,xteawl,xtewc,xtech,xtecc,xtep,xteuw,xteh,xtem,xtens),axis=1)\n",
        "     from sklearn.metrics import f1_score\n",
        "     xte_final=[test_pad_text,test_pad_keyword,xte_com]\n",
        "     a=model.predict(xte_final)\n",
        "     ypred=[]\n",
        "     for i in a:\n",
        "       if i >= 0.5:\n",
        "         ypred.append(1)\n",
        "       else:\n",
        "          ypred.append(0)\n",
        "\n",
        "     return f1_score(ytest,ypred)"
      ],
      "metadata": {
        "id": "5MaOH1B1ZcHQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=pd.read_csv(\"f2.csv\")"
      ],
      "metadata": {
        "id": "KrEF7iukdoga"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=final_2(\"f2.csv\")\n",
        "print(\" \")\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKwaMbkJcVOG",
        "outputId": "04b112a8-3348-4720-86d3-b0561320afaa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7552/7552 [00:01<00:00, 5620.70it/s]\n",
            "WARNING:tensorflow:5 out of the last 82 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2691ee2f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "0.7555120039196472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yY9eDhcOdRwe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}