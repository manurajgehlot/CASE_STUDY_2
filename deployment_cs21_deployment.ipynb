{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EyY0V27URe3"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n",
        "!pip install ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohpi4W497K37",
        "outputId": "7c27f32b-6e56-4178-bf80-5badf8bae484"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 28.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=3ce7c8ab9a2e74145507be7d1e7ce9877ab514fcd488a6c8476f68c30c347324\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reMkD_gvpyVv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --header=\"Host: doc-08-0s-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en;q=0.9,en-US;q=0.8,hi;q=0.7\" --header=\"Cookie: AUTH_r22p3bnqkdovpg2br3nin4vq452ouvbq_nonce=gfn85ltscbkpa\" --header=\"Connection: keep-alive\" \"https://doc-08-0s-docs.googleusercontent.com/docs/securesc/6t6i02ovbgfj1os3719d8cb4ahfv0ts1/v5ljbjklkenlifmcassloajn1pk68tma/1662550125000/00484516897554883881/13665802893543793158/1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_?e=download&ax=AI9vYm7qlXf2NIJb6PJxkRLLh8F-u1cMwD-GkHl_sYgex2WnFqKpdN4DjBRadE8XNMPS0m-TqthYG5kMuVeQmNqdcG7H8XBqvZGm4cGFOw9oSZphXaxr70kr-Yq9xy1lO_ayM2FC-vAQk4VEOW97w5m2es-tFKJOPRTcYppCkeS8tvuOPMtq-9oaM-5lyXfvSk9K3d1RwiP5gd1FpQm2Nsbx09kUo6I9Q2s-yG5ELQZ_VykaR6tqspdmiHHmF3aE6yA-bAaYTxapSZJk0RPiBH2gjPZ00ozudc0q8kxQ275kI5gOc2FImXgZ-jTQbJw0y65yRuvPsnc1e70S-lCgidc_ebKgMkskBKza5FlX5wr0X9vtu8_PToNjOaaovuLWJTqZsGs20Z1_PtiwRSgcbE_b0JuRyBpo1qF8nh4P3e0EACrZuYnR_BqmrYp_ptOhbQ5s-iEebEaEHijHjRgU6vMaa0zfvpXsg5-l53Kw486TT0OahFZN5fCnzkuQWt59XbI8DMIG5d51Li00OqI1lBjMoXzaxFv1jCYYjHjf9U59Akrq0RoBUEI2PI0gKcdGMdp7-p9tscWZrAmMWz9uF9rkB1fVPy0sQ70wHZhm55LVSx2C5vRCJyg3pNokg-LgB9GyZ89t60mYdXqyJaJqz0T9uP7rE_qJlUQZSU4FPSKTFpxPD58gBlD38KAMssqMWDphugK_iCXUW508Tw7WfgYMHcyE050mQ-_HevTRajRjC8U_8ynB1j4YIWTxUdc5Wawselkhdw5RDm9-xwt28i4qLT8Km8Ts0LHK1y37Lz9IbMQY1kTDOVo1pA&uuid=279eb960-e3e3-4166-afd1-42c44794c14b&authuser=0&nonce=gfn85ltscbkpa&user=13665802893543793158&hash=00f9r77809asdn61m47tj05e33080rvt\" -c -O 'glove_vectors'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-eerM_8tpVU",
        "outputId": "d4fad670-1552-4456-effc-8a6909462fb5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-07 11:29:48--  https://doc-08-0s-docs.googleusercontent.com/docs/securesc/6t6i02ovbgfj1os3719d8cb4ahfv0ts1/v5ljbjklkenlifmcassloajn1pk68tma/1662550125000/00484516897554883881/13665802893543793158/1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_?e=download&ax=AI9vYm7qlXf2NIJb6PJxkRLLh8F-u1cMwD-GkHl_sYgex2WnFqKpdN4DjBRadE8XNMPS0m-TqthYG5kMuVeQmNqdcG7H8XBqvZGm4cGFOw9oSZphXaxr70kr-Yq9xy1lO_ayM2FC-vAQk4VEOW97w5m2es-tFKJOPRTcYppCkeS8tvuOPMtq-9oaM-5lyXfvSk9K3d1RwiP5gd1FpQm2Nsbx09kUo6I9Q2s-yG5ELQZ_VykaR6tqspdmiHHmF3aE6yA-bAaYTxapSZJk0RPiBH2gjPZ00ozudc0q8kxQ275kI5gOc2FImXgZ-jTQbJw0y65yRuvPsnc1e70S-lCgidc_ebKgMkskBKza5FlX5wr0X9vtu8_PToNjOaaovuLWJTqZsGs20Z1_PtiwRSgcbE_b0JuRyBpo1qF8nh4P3e0EACrZuYnR_BqmrYp_ptOhbQ5s-iEebEaEHijHjRgU6vMaa0zfvpXsg5-l53Kw486TT0OahFZN5fCnzkuQWt59XbI8DMIG5d51Li00OqI1lBjMoXzaxFv1jCYYjHjf9U59Akrq0RoBUEI2PI0gKcdGMdp7-p9tscWZrAmMWz9uF9rkB1fVPy0sQ70wHZhm55LVSx2C5vRCJyg3pNokg-LgB9GyZ89t60mYdXqyJaJqz0T9uP7rE_qJlUQZSU4FPSKTFpxPD58gBlD38KAMssqMWDphugK_iCXUW508Tw7WfgYMHcyE050mQ-_HevTRajRjC8U_8ynB1j4YIWTxUdc5Wawselkhdw5RDm9-xwt28i4qLT8Km8Ts0LHK1y37Lz9IbMQY1kTDOVo1pA&uuid=279eb960-e3e3-4166-afd1-42c44794c14b&authuser=0&nonce=gfn85ltscbkpa&user=13665802893543793158&hash=00f9r77809asdn61m47tj05e33080rvt\n",
            "Resolving doc-08-0s-docs.googleusercontent.com (doc-08-0s-docs.googleusercontent.com)... 172.253.115.132, 2607:f8b0:4004:c06::84\n",
            "Connecting to doc-08-0s-docs.googleusercontent.com (doc-08-0s-docs.googleusercontent.com)|172.253.115.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127506004 (122M) [application/octet-stream]\n",
            "Saving to: ‘glove_vectors’\n",
            "\n",
            "glove_vectors       100%[===================>] 121.60M  72.5MB/s    in 1.7s    \n",
            "\n",
            "2022-09-07 11:29:50 (72.5 MB/s) - ‘glove_vectors’ saved [127506004/127506004]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "eL3R7YISwUhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1Xerrsa9D4g",
        "outputId": "6a470abd-6e23-432c-abce-26701d1a94f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import pickle\n",
        "import string \n",
        "import re\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "with open(\"glove_vectors\",'rb') as f:\n",
        "  embeddings_index=pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "model=pickle.load(open('file.pkl','rb'))\n",
        "st.title(\"TWEET CLASSIFICATION AS DISASTER OR NOT DISASTER\")\n",
        "uploaded_file=st.file_uploader(\"Choose a File\")\n",
        "if st.button(\"predict\"):\n",
        "   if uploaded_file is not None:\n",
        "     df=pd.read_csv(uploaded_file,index_col=\"Unnamed: 0\")\n",
        "     df.drop(columns=\"location\",inplace=True)\n",
        "     df.drop(columns=\"target\",inplace=True)\n",
        "    \n",
        "     df.dropna(inplace=True)\n",
        "     df.set_index(\"id\",inplace=True)\n",
        "     def word_count(text):\n",
        "         b=len(text.split())\n",
        "         return b\n",
        "\n",
        "     def count_char(text):\n",
        "         a=0\n",
        "         for i in text.split():\n",
        "           c=len(i)\n",
        "           a+=c   \n",
        "         return a\n",
        "          \n",
        "     def cap_count_char(text):\n",
        "         temp=0\n",
        "         for i in text:\n",
        "           if i.isupper():\n",
        "             temp+=1\n",
        "         return temp\n",
        "\n",
        "      \n",
        "     def punct(text):\n",
        "         p=string.punctuation\n",
        "         c=0\n",
        "         for i in p:\n",
        "           if i in text:\n",
        "              c+=1\n",
        "         return c\n",
        "\n",
        "     def uni_word(text):\n",
        "         uw=len(set(text.split()))\n",
        "         return uw\n",
        "        \n",
        "      \n",
        "     def hash(text):\n",
        "         m=re.findall(r'#\\w[A-Za-z0-9]*',text)\n",
        "         return len(m)\n",
        "\n",
        "     def ment(text):\n",
        "         m=re.findall(r'@\\w[A-Za-z0-9]*',text)\n",
        "         return len(m)\n",
        "\n",
        "     def number_sentence(text):\n",
        "         ns=nltk.sent_tokenize(text)\n",
        "         return len(ns)\n",
        "\n",
        "     df[\"ment\"]=df['text'].apply(lambda x:ment(x))\n",
        "     df[\"number_sentence\"]=df['text'].apply(lambda x:number_sentence(x))\n",
        "     df[\"hash\"]=df[\"text\"].apply(lambda x:hash(x))\n",
        "     df[\"uni_word\"]=df[\"text\"].apply(lambda x:uni_word(x))\n",
        "     df[\"punct\"]=df[\"text\"].apply(lambda x:punct(x))\n",
        "     df[\"cap_count_char\"]=df[\"text\"].apply(lambda x:cap_count_char(x))\n",
        "     df[\"count_char\"]=df[\"text\"].apply(lambda x:count_char(x))\n",
        "     df[\"word_count\"]=df[\"text\"].apply(lambda x:word_count(x))\n",
        "     df['average_word_length']=df['count_char']/df['word_count']\n",
        "     df['average_sentence_length']=df['word_count']/df['number_sentence']\n",
        "     df['unique_word_ratio']=df['uni_word']/df['word_count']\n",
        "     \n",
        "     lemmatizer = WordNetLemmatizer()\n",
        "     stopword=set(stopwords.words(\"english\"))\n",
        "     ps = PorterStemmer()\n",
        "     preprocessed_text=[]\n",
        "     for i in tqdm(df.text.values):\n",
        "       i=re.sub(r\"http\\S+\",\"\",i)\n",
        "       i=re.sub(\"\\S*\\d\\S*\",'',i)\n",
        "       i=re.sub(\"[^A-Za-z]+\",' ',i)\n",
        "       wk = word_tokenize(i)##https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "       i = ' '.join(w for w in wk if not w.lower() in stopword)\n",
        "        \n",
        "       preprocessed_text.append(i.strip())\n",
        "\n",
        "\n",
        "     df.text=preprocessed_text\n",
        "\n",
        "    \n",
        "     key=[]\n",
        "     for i in df[\"keyword\"]:\n",
        "         i=str(i)\n",
        "         a=re.sub(\"[^A-Za-z]+\",'',i)\n",
        "         key.append(a)\n",
        "     df[\"keyword\"]=key\n",
        "     \n",
        "     xtest=df.copy()\n",
        "     \n",
        "     vectorizer=pickle.load(open('vectorizer.pkl','rb'))\n",
        "     xtetex=vectorizer.transform(xtest[\"text\"].values)\n",
        "\n",
        "     labelencoder=pickle.load(open('labelencoder.pkl','rb'))\n",
        "     xtekwd=labelencoder.transform(xtest['keyword'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm11=pickle.load(open('norm11.pkl','rb'))\n",
        "     xteuwr=norm11.transform(xtest['unique_word_ratio'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm10=pickle.load(open('norm10.pkl','rb'))\n",
        "     xteasl=norm10.transform(xtest['average_sentence_length'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm9=pickle.load(open('norm9.pkl','rb'))\n",
        "     xteawl=norm9.transform(xtest['average_word_length'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm8=pickle.load(open('norm8.pkl','rb'))\n",
        "     xtewc=norm8.transform(xtest[\"word_count\"].values.reshape(-1,1))\n",
        "\n",
        "     norm7=pickle.load(open('norm7.pkl','rb'))\n",
        "     xtech=norm7.transform(xtest[\"count_char\"].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "     norm6=pickle.load(open('norm6.pkl','rb'))\n",
        "     xtecc=norm6.transform(xtest[\"cap_count_char\"].values.reshape(-1,1))\n",
        "\n",
        "     norm5=pickle.load(open('norm5.pkl','rb'))\n",
        "     xtep=norm5.transform(xtest[\"punct\"].values.reshape(-1,1))\n",
        "\n",
        "     norm4=pickle.load(open('norm4.pkl','rb'))\n",
        "     xteuw=norm4.transform(xtest[\"uni_word\"].values.reshape(-1,1))\n",
        "\n",
        "     norm3=pickle.load(open('norm3.pkl','rb'))\n",
        "     xteh=norm3.transform(xtest[\"hash\"].values.reshape(-1,1))\n",
        "\n",
        "     norm1=pickle.load(open('norm1.pkl','rb'))\n",
        "     xtem=norm1.transform(xtest[\"ment\"].values.reshape(-1,1))\n",
        "\n",
        "     norm2=pickle.load(open('norm2.pkl','rb'))\n",
        "     xtens=norm2.transform(xtest[\"number_sentence\"].values.reshape(-1,1))\n",
        "     xte_t=xtest[\"text\"]\n",
        "     \n",
        "     tokenizer=pickle.load(open('tokenizer11.pkl','rb'))\n",
        "     vocab_size=len(tokenizer.word_index)+1\n",
        "     maxlen_text=21\n",
        "     embedding_matrix=np.zeros((vocab_size,300))\n",
        "     for word,i in tokenizer.word_index.items():\n",
        "        embedding_vector=embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          embedding_matrix[i]=embedding_vector\n",
        "\n",
        "     test_seq_text=tokenizer.texts_to_sequences(xte_t)\n",
        "     test_pad_text=pad_sequences(test_seq_text,maxlen=maxlen_text,padding='pre')\n",
        "\n",
        "\n",
        "     xte_k=xtest[\"keyword\"]\n",
        "     tokenizer=pickle.load(open('tokenizer21.pkl','rb'))\n",
        "     vocab_size_key=len(tokenizer.word_index)+1\n",
        "     maxlen_keyword=1\n",
        "     test_seq_keyword=tokenizer.texts_to_sequences(xte_k)\n",
        "     test_pad_keyword=pad_sequences(test_seq_keyword,maxlen=maxlen_keyword,padding='pre')\n",
        "\n",
        "     xte_com=np.concatenate((xteuwr,xteasl,xteawl,xtewc,xtech,xtecc,xtep,xteuw,xteh,xtem,xtens),axis=1)\n",
        "     from sklearn.metrics import f1_score\n",
        "     xte_final=[test_pad_text,test_pad_keyword,xte_com]\n",
        "     y_p=model.predict(xte_final)\n",
        "     if y_p >=0.5:\n",
        "       st.write(\"IT IS A DISASTER TWEET\")\n",
        "     else:\n",
        "       st.write(\"IT IS NOT A  DISASTER TWEET\")\n",
        "     \n",
        "                             \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzzBiOA8wedt",
        "outputId": "2c192be0-f5a7-4177-9203-b6cf67ee23dc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('2DQlpVpnhSIFXKSBhEU0y6h4Qw6_4UAwqZpWmhh5EbLzqudSz')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QadP-fZu6j7K",
        "outputId": "fe0254fb-1527-4196-b6cd-8f9f532cab22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.7/dist-packages/pyngrok/bin/ngrok\n",
            "2022-09-07 07:50:24.276 INFO    pyngrok.process: Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.7/dist-packages/pyngrok/bin/ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbIOo3lk7bS9",
        "outputId": "2c4b738d-dabe-4fe9-f0cc-d01f4a1e848e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Killing ngrok process: 1511\n",
            "2022-09-07 11:36:37.488 INFO    pyngrok.process: Killing ngrok process: 1511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 80 &\n",
        "url=ngrok.connect(port = '80')\n",
        "print(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhxddH_E7g2Q",
        "outputId": "22a18819-b1fe-4116-efbf-9640df1346b2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-80-444befbf-1288-4358-992f-f23a49644dcc\n",
            "2022-09-07 11:36:40.628 INFO    pyngrok.ngrok: Opening tunnel named: http-80-444befbf-1288-4358-992f-f23a49644dcc\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2022-09-07 11:36:40.707 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2022-09-07 11:36:40.714 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2022-09-07 11:36:40.725 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2022-09-07 11:36:40.737 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2022-09-07 11:36:40.831 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"client session established\" obj=csess id=ca27f204a5e7\n",
            "2022-09-07 11:36:40.839 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"client session established\" obj=csess id=ca27f204a5e7\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=/api/tunnels id=2395a3bf88d9c106\n",
            "2022-09-07 11:36:40.865 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=/api/tunnels id=2395a3bf88d9c106\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=/api/tunnels id=2395a3bf88d9c106 status=200 dur=450.053µs\n",
            "2022-09-07 11:36:40.879 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=/api/tunnels id=2395a3bf88d9c106 status=200 dur=450.053µs\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=/api/tunnels id=e3cfcc6952cf703a\n",
            "2022-09-07 11:36:40.889 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=/api/tunnels id=e3cfcc6952cf703a\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=/api/tunnels id=e3cfcc6952cf703a status=200 dur=166.725µs\n",
            "2022-09-07 11:36:40.907 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=/api/tunnels id=e3cfcc6952cf703a status=200 dur=166.725µs\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=/api/tunnels id=b0790b6b43bbb126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"http://c652-35-245-253-139.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-07 11:36:40.920 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=/api/tunnels id=b0790b6b43bbb126\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-80-444befbf-1288-4358-992f-f23a49644dcc (http)\" addr=http://localhost:80 url=http://c652-35-245-253-139.ngrok.io\n",
            "2022-09-07 11:36:40.936 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-80-444befbf-1288-4358-992f-f23a49644dcc (http)\" addr=http://localhost:80 url=http://c652-35-245-253-139.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-80-444befbf-1288-4358-992f-f23a49644dcc addr=http://localhost:80 url=https://c652-35-245-253-139.ngrok.io\n",
            "2022-09-07 11:36:40.948 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-80-444befbf-1288-4358-992f-f23a49644dcc addr=http://localhost:80 url=https://c652-35-245-253-139.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=/api/tunnels id=b0790b6b43bbb126 status=201 dur=49.048057ms\n",
            "2022-09-07 11:36:40.967 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=/api/tunnels id=b0790b6b43bbb126 status=201 dur=49.048057ms\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=\"/api/tunnels/http-80-444befbf-1288-4358-992f-f23a49644dcc (http)\" id=5d9e5f8def736d24\n",
            "2022-09-07 11:36:40.978 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=start pg=\"/api/tunnels/http-80-444befbf-1288-4358-992f-f23a49644dcc (http)\" id=5d9e5f8def736d24\n",
            "INFO:pyngrok.process.ngrok:t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=\"/api/tunnels/http-80-444befbf-1288-4358-992f-f23a49644dcc (http)\" id=5d9e5f8def736d24 status=200 dur=178.977µs\n",
            "2022-09-07 11:36:40.996 INFO    pyngrok.process.ngrok: t=2022-09-07T11:36:40+0000 lvl=info msg=end pg=\"/api/tunnels/http-80-444befbf-1288-4358-992f-f23a49644dcc (http)\" id=5d9e5f8def736d24 status=200 dur=178.977µs\n"
          ]
        }
      ]
    }
  ]
}